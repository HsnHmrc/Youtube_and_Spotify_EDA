{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5201951,"sourceType":"datasetVersion","datasetId":3025170}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 📊🎵 YouTube and Spotify Data Analysis  \n#### This project aims to explore music listening trends and user behavior by analyzing datasets from YouTube and Spotify platforms. A comprehensive analysis was conducted using various musical attributes (e.g., `Energy`, `Valence`, `Tempo`) and engagement metrics (e.g., `Views`, `Likes`). \n#### The analysis also includes extensive and important insights and advice for companies.\n\n---\n\n## 🚀 Key Highlights  \n- **Most Popular Keys:** 🎹 Tracks in the keys of C (C#), F# (Fa#) and A# (La#) and at 120-150 BPM reached the highest number of views.   \n- **Musical Features and Engagement:** 🎧 Relationships between features like `Tempo`, `Energy`, and `Speechiness` and metrics like views and likes were examined.  \n- **Licensed & Official Video:** 🏷️ The impact of these content types on view counts was analyzed in detail.  \n- **Outlier Cleaning:** 🧹 All outliers were removed across features to enhance the accuracy of the analysis.\n- **Filling Missing Values:** 💾 Empty values that can be filled with the RandomForestRegressor model are filled. \n- **Categorization:** 📦 Continuous variables like `Energy`, `Valence`, and `Tempo` were grouped for clearer insights.  \n\n---\n\n## 🧐 Questions Answered  \n- Which musical features influence a track's popularity?  \n- How do licensed and official video content types shape viewership patterns?  \n- Are specific `Tempo` and `Energy` levels more appealing to listeners?\n- What are the factors that influence the listener?\n- Which are the View based Top 10?  \n\n---\n\n## 📋 Techniques Used  \n- **Data Cleaning and Preparation:** Missing and outlier values were addressed prior to analysis(Pandas, RandomForestRegressor, IQR). \n- **Exploratory Data Analysis (EDA):** 📊 Detailed analysis was performed using visualization tools.  \n- **Categorization:** Continuous variables were divided into ranges for better comparisons.  \n- **Visualizations:** Metrics were visualized using pie charts, histograms, heat maps and scatter plots.  ","metadata":{}},{"cell_type":"markdown","source":"# ℹ️  ****Columns Information****\n\n**Track**:  \r\nName of the song, as visible on the Spotify platform.\r\n\r\n**Artist**:  \r\nName of the artist.\r\n\r\n**Url_spotify**:  \r\nThe URL of the artist.\r\n\r\n**Album**:  \r\nThe album in which the song is contained on Spotify.\r\n\r\n**Album_type**:  \r\nIndicates if the song is released on Spotify as a single or contained in an album.\r\n\r\n**Uri**:  \r\nA Spotify link used to find the song through the API.\r\n\r\n**Danceability**:  \r\nDescribes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable, and 1.0 is most danceable.\r\n\r\n**Energy**:  \r\nA measure from 0.0 to 1.0 that represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\r\n\r\n**Key**:  \r\nThe key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\r\n\r\n**Loudness**:  \r\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Values typically range between -60 and 0 dB.\r\n\r\n**Speechiness**:  \r\nDetects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g., talk show, audiobook, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words, while values between 0.33 and 0.66 describe tracks that may contain both music and speech. Values below 0.33 most likely represent music and other non-speech-like tracks.\r\n\r\n**Acousticness**:  \r\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\r\n\r\n**Instrumentalness**:  \r\nPredicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 represent instrumental tracks, with higher confidence as the value approaches 1.0.\r\n\r\n**Liveness**:  \r\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\r\n\r\n**Valence**:  \r\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g., happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g., sad, depressed, angry).\r\n\r\n**Tempo**:  \r\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\r\n\r\n**Duration_ms**:  \r\nThe duration of the track in milliseconds.\r\n\r\n**Stream**:  \r\nNumber of streams of the song on Spotify.\r\n\r\n**Url_youtube**:  \r\nURL of the video linked to the song on YouTube, if it exists.\r\n\r\n**Title**:  \r\nTitle of the videoclip on YouTube.\r\n\r\n**Channel**:  \r\nName of the channel that published the video.\r\n\r\n**Views**:  \r\nNumber of views.\r\n\r\n**Likes**:  \r\nNumber of likes.\r\n\r\n**Comments**:  \r\nNumber of comments.\r\n\r\n**Description**:  \r\nDescription of the video on YouTube.\r\n\r\n**Licensed**:  \r\nIndicates whether the video represents licensed content, meaning that the content was uploaded to a channel linked to a YouTube content partner and then claimed by that partner.\r\n\r\n**Official_video**:  \r\nBoolean value indicating if the video found is the official video of the song.fficial video of the song.","metadata":{}},{"cell_type":"markdown","source":"# ****Libraries****","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom matplotlib.ticker import FuncFormatter\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:18.416589Z","iopub.execute_input":"2024-11-22T12:24:18.417018Z","iopub.status.idle":"2024-11-22T12:24:21.410238Z","shell.execute_reply.started":"2024-11-22T12:24:18.416975Z","shell.execute_reply":"2024-11-22T12:24:21.409103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Creating Dataset (Youtube and Spotify)****","metadata":{}},{"cell_type":"code","source":"# After creating the necessary libraries, let's start by creating the data set.\n\n# Importing dataset\ndf_ = pd.read_csv(filepath_or_buffer = \"/kaggle/input/spotify-and-youtube/Spotify_Youtube.csv\")\ndf = df_.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:21.412375Z","iopub.execute_input":"2024-11-22T12:24:21.413027Z","iopub.status.idle":"2024-11-22T12:24:22.697701Z","shell.execute_reply.started":"2024-11-22T12:24:21.412978Z","shell.execute_reply":"2024-11-22T12:24:22.696598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Creating Empty Data (%2)****","metadata":{}},{"cell_type":"code","source":"#Let's create blank values on dataset.\n\nimport random\n\ndef add_random_missing_values(dataframe: pd.DataFrame,\n                              missing_rate: float = 0.02,\n                              seed: random = 42) -> pd.DataFrame:\n    \n    # Get copy of dataframe\n    df_missing = dataframe.copy()\n\n    # Obtain size of dataframe and number total number of missing values\n    df_size = dataframe.size\n    num_missing = int(df_size * missing_rate)\n    \n    # Set seed\n    if seed:\n        random.seed(seed)\n\n    # Get random row and column indexes to turn them NaN\n    for _ in range(num_missing):\n        row_idx = random.randint(0, dataframe.shape[0] - 1)\n        col_idx = random.randint(0, dataframe.shape[1] - 1)\n\n        df_missing.iat[row_idx, col_idx] = np.nan\n        \n    return df_missing\n\ndf = add_random_missing_values(dataframe = df,\n                               missing_rate = 0.02)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:22.699236Z","iopub.execute_input":"2024-11-22T12:24:22.699699Z","iopub.status.idle":"2024-11-22T12:24:22.905948Z","shell.execute_reply.started":"2024-11-22T12:24:22.699649Z","shell.execute_reply":"2024-11-22T12:24:22.904818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Dataset review and overview****","metadata":{}},{"cell_type":"code","source":"\n# Let's check dataset's shapes.\n\nprint(f\"There are {df.shape[0]} rows and {df.shape[1]} columns in this dataset.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:22.908671Z","iopub.execute_input":"2024-11-22T12:24:22.909433Z","iopub.status.idle":"2024-11-22T12:24:22.914737Z","shell.execute_reply.started":"2024-11-22T12:24:22.909386Z","shell.execute_reply":"2024-11-22T12:24:22.913615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's make a general observation on the dataset. We can also do this observation with .head() and .tail() commands.\n# But I choose only the df command because it shows the head and tail data together.\n# I use the .T command because it makes observation easier when there are many columns.\n\ndf.T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:22.916054Z","iopub.execute_input":"2024-11-22T12:24:22.916477Z","iopub.status.idle":"2024-11-22T12:24:22.983894Z","shell.execute_reply.started":"2024-11-22T12:24:22.916413Z","shell.execute_reply":"2024-11-22T12:24:22.982589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's take a look at the columns in general.\n\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:22.985327Z","iopub.execute_input":"2024-11-22T12:24:22.985659Z","iopub.status.idle":"2024-11-22T12:24:23.035836Z","shell.execute_reply.started":"2024-11-22T12:24:22.985626Z","shell.execute_reply":"2024-11-22T12:24:23.034712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's check the number of Null values in dataset.\n# With the result of this code, we will have a general knowledge about Null values.\n# This will help us decide what to do next.\n\ndf.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.037069Z","iopub.execute_input":"2024-11-22T12:24:23.037412Z","iopub.status.idle":"2024-11-22T12:24:23.061095Z","shell.execute_reply.started":"2024-11-22T12:24:23.037381Z","shell.execute_reply":"2024-11-22T12:24:23.059800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# With the Describe command, let's check the quartiles, minimum and maximum values, and average of numerical data.\n# If we add include = “all” we will examine all columns, but this is not necessary for now. I only want to examine numeric columns.\n\ndf.describe().T\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.062983Z","iopub.execute_input":"2024-11-22T12:24:23.064490Z","iopub.status.idle":"2024-11-22T12:24:23.147248Z","shell.execute_reply.started":"2024-11-22T12:24:23.064363Z","shell.execute_reply":"2024-11-22T12:24:23.146072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At first look, I see that some of the values that should be between 0 and 1 are incorrect.\n# I will manipulate this with appropriate methods in the following process.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.148794Z","iopub.execute_input":"2024-11-22T12:24:23.149290Z","iopub.status.idle":"2024-11-22T12:24:23.154210Z","shell.execute_reply.started":"2024-11-22T12:24:23.149212Z","shell.execute_reply":"2024-11-22T12:24:23.153034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# With a short summary table, I want get a general overview and decide how much to delete or if I need to fill in.\n# With this table we will be able to see the percentage of missing values, the number of unique values, the data type and more.\n\nsummary_table = pd.DataFrame({\n    \"Data Type\" : df.dtypes,\n    \"Missing\" : df.isnull().sum(),\n    \"Missing %\" : (df.isnull().sum()/df.count())*100,\n    \"Unique\" : df.nunique(),\n    \"Count\" : df.count(),\n})\n\nsummary_table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.159142Z","iopub.execute_input":"2024-11-22T12:24:23.159546Z","iopub.status.idle":"2024-11-22T12:24:23.310251Z","shell.execute_reply.started":"2024-11-22T12:24:23.159511Z","shell.execute_reply":"2024-11-22T12:24:23.309183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Maipulation of The Data Set****","metadata":{}},{"cell_type":"code","source":"# In the numerical columns where I will make inferences, I see that Null values are around 2-6%.\n# I might prefer to exclude Null values here because the rate of Null values is very low.\n# But it is important to remember that in professional life, this decision is made together with our teammates or according to the upper limit set in the company.\n# We can start manipulating our dataset.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.311550Z","iopub.execute_input":"2024-11-22T12:24:23.311856Z","iopub.status.idle":"2024-11-22T12:24:23.316280Z","shell.execute_reply.started":"2024-11-22T12:24:23.311827Z","shell.execute_reply":"2024-11-22T12:24:23.315205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unnamed: 0 column is useless, so I remove it and reset the index.\n\ndf.drop(\"Unnamed: 0\", axis = 1, inplace = True)\ndf.reset_index(drop = True, inplace = True)\n\n# Firstly I converted milliseconds to minutes (1 min = 60000 ms) and removed the Duration_ms column because we don't need it.\ndf[\"Duration_min\"] = df[\"Duration_ms\"] / 60000\ndf.drop(columns = \"Duration_ms\", inplace = True)\n\n# And than I identify outliers.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.317493Z","iopub.execute_input":"2024-11-22T12:24:23.317809Z","iopub.status.idle":"2024-11-22T12:24:23.347889Z","shell.execute_reply.started":"2024-11-22T12:24:23.317776Z","shell.execute_reply":"2024-11-22T12:24:23.346568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Identification of Outliers****","metadata":{}},{"cell_type":"markdown","source":"## For 0-1 Range","metadata":{}},{"cell_type":"code","source":"# Let's identify the columns with a value between 0-1.\ncolumns_to_check = ['Danceability', 'Energy', 'Speechiness', 'Acousticness', \n                    'Instrumentalness', 'Liveness', 'Valence']\n\n# Checking values outside the 0-1 range and printing.\nfor col in columns_to_check:\n    out_of_range = df[(df[col] < 0) | (df[col] > 1)]\n    print(f\"In the {col} column, {len(out_of_range)} values were found outside the 0-1 range.\")\n    if not out_of_range.empty:\n        print(out_of_range[[col]])\n    print('-' * 74)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.349744Z","iopub.execute_input":"2024-11-22T12:24:23.350110Z","iopub.status.idle":"2024-11-22T12:24:23.365566Z","shell.execute_reply.started":"2024-11-22T12:24:23.350075Z","shell.execute_reply":"2024-11-22T12:24:23.364468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Since there are no outliers in the columns I checked, I will not perform data cleaning. \n# For the other columns I will check, I will determine outlier values with the IQR method. \n# Since I will perform analysis on these columns, I will perform data cleaning by selecting only these columns.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.366814Z","iopub.execute_input":"2024-11-22T12:24:23.367234Z","iopub.status.idle":"2024-11-22T12:24:23.381583Z","shell.execute_reply.started":"2024-11-22T12:24:23.367185Z","shell.execute_reply":"2024-11-22T12:24:23.380203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IQR Method","metadata":{}},{"cell_type":"code","source":"# Let's check dataset.\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:23.382972Z","iopub.execute_input":"2024-11-22T12:24:23.383369Z","iopub.status.idle":"2024-11-22T12:24:23.439443Z","shell.execute_reply.started":"2024-11-22T12:24:23.383320Z","shell.execute_reply":"2024-11-22T12:24:23.438177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now let's check the number of outlier values.\n# I prefer the IQR method to determine outlier values. \n\n# Here I create the necessary function to determine the outlier values with IQR values.\ndef find_outliers_iqr(data, column):\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n\n    # I create the variables Upper and Lower bonds.\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # I create variables to identify outliers\n    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n    return outliers, lower_bound, upper_bound\n\n\ncolumns_to_check = [\"Duration_min\", \"Likes\", \"Views\", \"Tempo\"]\n\n# With Scatterplot I create a graph to show outliers and print their numbers.\n\nplt.figure(figsize = (12, 8))\nfor i, col in enumerate(columns_to_check, 1):\n    # Calculation of lower and upper limits\n    outliers, lower, upper = find_outliers_iqr(df, col)\n\n    # I also create normal values with variables to get a better visualization in the graph.\n    normal_values = df[(df[col] >= lower) & (df[col] <= upper)]\n\n    # Creating scatterplot\n    plt.subplot(2, 2, i)\n    plt.scatter(normal_values.index, normal_values[col], label = \"Normal\", alpha = 0.6, c = \"green\")\n    plt.scatter(outliers.index, outliers[col], label = \"Outlier\", alpha=0.6, c = \"red\")\n    plt.title(f\"{col} Scatterplot\")\n    plt.ylabel(col)\n    plt.legend()\n\n    # I print the lower and upper bounds with the total number of outliers.\n    print(f\"Total number of outliers in column {col}: {len(outliers)}\")\n    print(f\"Lower limit: {lower}, Upper limit: {upper}\")\n    print(\"-\" * 59)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:23:19.946038Z","iopub.execute_input":"2024-11-22T13:23:19.947394Z","iopub.status.idle":"2024-11-22T13:23:22.885173Z","shell.execute_reply.started":"2024-11-22T13:23:19.947346Z","shell.execute_reply":"2024-11-22T13:23:22.883872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The outlier values I obtained for Duration_min, Likes, Views columns were not satisfactory. \n# Because when I look at the actual values, I see that there are 8 billion Views and 54 million Likes. Therefore, I will not delete data on Likes and Views columns.\n# For the Duration_min column, I will set a value myself and delete accordingly because there are songs longer than 6-7 minutes.\n# For the Tempo column, I want to examine in a boxplot graph and make data extraction in this way.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:25.380389Z","iopub.execute_input":"2024-11-22T12:24:25.380826Z","iopub.status.idle":"2024-11-22T12:24:25.387006Z","shell.execute_reply.started":"2024-11-22T12:24:25.380780Z","shell.execute_reply":"2024-11-22T12:24:25.385884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For the tempo column, we do the IQR operations again, only this time the chart will be different.\ndef find_outliers_iqr(data, column):\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n    return outliers, lower_bound, upper_bound\n\ntempo_outliers, tempo_lower, tempo_upper = find_outliers_iqr(df, \"Tempo\")\n\n# Let's visualize Outliers with Boxplot.\nplt.figure(figsize = (8, 6))\nsns.boxplot(data = df, y = \"Tempo\", palette = \"Greens\")\nplt.title(\"Tempo Column - Outliers\")\nplt.ylabel(\"Tempo\")\nplt.show()\n\n# The code we use to print the outlier number.\nprint(f\"Total number of outliers in the Tempo column: {len(tempo_outliers)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:25.388069Z","iopub.execute_input":"2024-11-22T12:24:25.388463Z","iopub.status.idle":"2024-11-22T12:24:25.557388Z","shell.execute_reply.started":"2024-11-22T12:24:25.388422Z","shell.execute_reply":"2024-11-22T12:24:25.556267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I have found that there are 63 outlier values in the tempo column.\n# Let's remove outlier values from dataset.\n\ndf = df[(df['Tempo'] >= tempo_lower) & (df['Tempo'] <= tempo_upper)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:25.558646Z","iopub.execute_input":"2024-11-22T12:24:25.559079Z","iopub.status.idle":"2024-11-22T12:24:25.573008Z","shell.execute_reply.started":"2024-11-22T12:24:25.559031Z","shell.execute_reply":"2024-11-22T12:24:25.571855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To determine the outlier values in Duration_min, I will scattering them among themselves. \n# I chose these columns because I will probably do my analysis with one of these 2 columns.\n\n# With this command I set the tables size.\nplt.figure(figsize = (12, 6))\n\n# Likes vs Duration_min\nplt.subplot(1, 2, 1)\nplt.scatter(df[\"Likes\"], df[\"Duration_min\"], color = \"green\", alpha = 0.5)\nplt.title(\"Likes vs Duration_min\")\nplt.xlabel(\"Likes\")\nplt.ylabel(\"Duration (min)\")\n\n# Views vs Duration_min\nplt.subplot(1, 2, 2)\nplt.scatter(df[\"Views\"], df[\"Duration_min\"], color = \"midnightblue\", alpha = 0.5)\nplt.title(\"Views vs Duration_min\")\nplt.xlabel(\"Views\")\nplt.ylabel(\"Duration (min)\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:25.574373Z","iopub.execute_input":"2024-11-22T12:24:25.574697Z","iopub.status.idle":"2024-11-22T12:24:26.171250Z","shell.execute_reply.started":"2024-11-22T12:24:25.574666Z","shell.execute_reply":"2024-11-22T12:24:26.169963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I see a similarity here, but I will address this in the Correlation Matrix section.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.172827Z","iopub.execute_input":"2024-11-22T12:24:26.173310Z","iopub.status.idle":"2024-11-22T12:24:26.178602Z","shell.execute_reply.started":"2024-11-22T12:24:26.173259Z","shell.execute_reply":"2024-11-22T12:24:26.177458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's print the numbers above the values specified for the Duration_min column.\nduration_thresholds = [20, 15, 10, 7]\nfor threshold in duration_thresholds:\n    count = (df[\"Duration_min\"] > threshold).sum()\n    print(f\"Number of values above {threshold} minutes for the Duration_min column: {count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.179920Z","iopub.execute_input":"2024-11-22T12:24:26.180272Z","iopub.status.idle":"2024-11-22T12:24:26.201558Z","shell.execute_reply.started":"2024-11-22T12:24:26.180206Z","shell.execute_reply":"2024-11-22T12:24:26.200181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Given the facts and the significant increase after 10 minutes, I decided that value of 10 minutes for the Duration_min column was appropriate.\n\n# I remove those with more than 10 minutes from the data set.\ndf = df[df[\"Duration_min\"] <= 10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.203045Z","iopub.execute_input":"2024-11-22T12:24:26.203438Z","iopub.status.idle":"2024-11-22T12:24:26.226523Z","shell.execute_reply.started":"2024-11-22T12:24:26.203403Z","shell.execute_reply":"2024-11-22T12:24:26.225087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In addition, I would like to point out that we could have chosen to shift the outlier values by one digit or to preserve them by applying other operations. \n# Since I did not perform this analysis with a team, I chose to preserve the accuracy of the data by removing the values.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.227955Z","iopub.execute_input":"2024-11-22T12:24:26.228354Z","iopub.status.idle":"2024-11-22T12:24:26.239169Z","shell.execute_reply.started":"2024-11-22T12:24:26.228317Z","shell.execute_reply":"2024-11-22T12:24:26.238032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Identifying and Filling Missing Data (With  Random Forest Regressor)****","metadata":{}},{"cell_type":"code","source":"# Let's see how much data is left and check the final status with the summary table\n\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.240658Z","iopub.execute_input":"2024-11-22T12:24:26.241281Z","iopub.status.idle":"2024-11-22T12:24:26.258544Z","shell.execute_reply.started":"2024-11-22T12:24:26.241204Z","shell.execute_reply":"2024-11-22T12:24:26.257319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary_table = pd.DataFrame({\n    \"Data Type\" : df.dtypes,\n    \"Missing\" : df.isnull().sum(),\n    \"Missing %\" : (df.isnull().sum()/df.count())*100,\n    \"Unique\" : df.nunique(),\n    \"Count\" : df.count(),\n})\n\nsummary_table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.260024Z","iopub.execute_input":"2024-11-22T12:24:26.260463Z","iopub.status.idle":"2024-11-22T12:24:26.377657Z","shell.execute_reply.started":"2024-11-22T12:24:26.260425Z","shell.execute_reply":"2024-11-22T12:24:26.376563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# What caught my attention here is that the data types are only object and float64. \n# Storing data in only 2 types will make our analysis a little easier.\n# Also, we can see the categorical values in a summary table.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.379256Z","iopub.execute_input":"2024-11-22T12:24:26.379595Z","iopub.status.idle":"2024-11-22T12:24:26.384143Z","shell.execute_reply.started":"2024-11-22T12:24:26.379562Z","shell.execute_reply":"2024-11-22T12:24:26.382841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We can get an overview by seeing the missing values in the numeric columns on the heat map.\n\n# I am creating a variable that returns only numeric columns. Remember, all numeric columns in float64 format! \nnumerical_columns = df.select_dtypes(include = \"float64\").columns\n\n# Creating graphs\nplt.figure(figsize = (10, 6))\nsns.heatmap(df[numerical_columns].isna(), cbar = False, cmap = \"plasma\")\nplt.title(\"Visualization of Missing Values\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:26.388757Z","iopub.execute_input":"2024-11-22T12:24:26.389128Z","iopub.status.idle":"2024-11-22T12:24:27.232552Z","shell.execute_reply.started":"2024-11-22T12:24:26.389093Z","shell.execute_reply":"2024-11-22T12:24:27.231273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  I choose Random Forest Regressor model for filling the missing values. Because:\n####    * * By averaging predictions from multiple decision trees, it reduces the risk of overfitting.\n####    * * Random Forest handles both linear and nonlinear relationships effectively.\n####    * * It can capture complex relationships among numerical features.\n####    * * Features with sufficient variance and non-missing values provide strong predictors for imputation.\n\n","metadata":{}},{"cell_type":"code","source":"# Random Forest Regressor for filling missing values.\n\n# I selected only numerical columns for the model.\nnumerical_columns = df.select_dtypes(include = \"float64\").columns\n\n# Creating a loop to fill in the missing values.\nfor col in numerical_columns:\n    # Here I select the rows that are not missing to train the model.\n    non_missing_data = df[df[col].notna()]\n    missing_data = df[df[col].isna()]\n\n    # If the column is completely empty, I use this code to skip it.\n    # There are no empty columns in our data set, but I try to use this command for habit.\n    if missing_data.empty:\n        continue\n\n    # Remove features (columns) that may create noise due to missing values.\n    # Feature engineering.\n    X = non_missing_data[numerical_columns].drop(columns = [col])\n    y = non_missing_data[col]\n\n    X_missing = missing_data[numerical_columns].drop(columns = [col])\n\n    # Excluding Features with Missing Values.\n    X = X.dropna(axis = 1)\n    X_missing = X_missing[X.columns]\n\n    # Let's standardize features.\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_missing_scaled = scaler.transform(X_missing)\n\n    # Let's train the model we created.\n    model = RandomForestRegressor(random_state=42)\n    model.fit(X_scaled, y)\n\n    # Let's predicting missing values.\n    predictions = model.predict(X_missing_scaled)\n\n    # And finally let's fill the missing values.\n    df.loc[df[col].isna(), col] = predictions\n\n# Let's check the results.\nprint(df.isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:24:27.240378Z","iopub.execute_input":"2024-11-22T12:24:27.240721Z","iopub.status.idle":"2024-11-22T12:28:25.751954Z","shell.execute_reply.started":"2024-11-22T12:24:27.240676Z","shell.execute_reply":"2024-11-22T12:28:25.750652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now that we can see that the empty values are filled, let's start the analysis.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:28:25.753663Z","iopub.execute_input":"2024-11-22T12:28:25.754122Z","iopub.status.idle":"2024-11-22T12:28:25.759852Z","shell.execute_reply.started":"2024-11-22T12:28:25.754072Z","shell.execute_reply":"2024-11-22T12:28:25.758478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Correlation Matrix, Counting Numerical Columns****","metadata":{}},{"cell_type":"code","source":"# For the correlation matrix we select only the columns that include numeric values. \n# When we examine the data, all numeric values in the table are in float64 format. \n# So we only select float64 format.\n\ndf_numeric = df.select_dtypes(include = [\"float64\"])\n\ndf_numeric.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:28:25.761058Z","iopub.execute_input":"2024-11-22T12:28:25.761410Z","iopub.status.idle":"2024-11-22T12:28:25.781902Z","shell.execute_reply.started":"2024-11-22T12:28:25.761369Z","shell.execute_reply":"2024-11-22T12:28:25.780851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's make a graphical index showing the distribution of numeric columns\n\n# Determine the value for the multiplier (M for Million, B for Billion) expressions shown in the header.\ndef get_scale_title(col_data, col_name):\n    max_val = col_data.max()\n    if max_val >= 1e9:\n        scale = \" (B)\"\n    elif max_val >= 1e6:\n        scale = \" (M)\"\n    else:\n        scale = \"\"\n    return f\"{col_name}{scale}\"\n\n# We logarithmically transformed the columns with large numbers so that the graph can be analyzed.\ndf_log = df_numeric.apply(lambda x: np.log1p(x) if x.max() > 1e5 else x)\n\n# Let's creat graphs with subplots.\nplt.figure(figsize = (16, 12))\nfor i, col in enumerate(df_log, 1):\n    plt.subplot(4, 4, i)\n    \n    # Special settings for Instrumentalness (0.0 - 0.1 range approximately 17500).\n    # Only for Instrumentalness, I set the y-axis limit to 1000.\n    if col == \"Instrumentalness\":\n        sns.histplot(df[col], kde = True, binwidth = 0.05)\n        plt.ylim(0, 1000)\n    else:\n        sns.histplot(df_log[col], kde = True)\n    \n    # The codes we use to show the header and the multiplier scale.\n    title = get_scale_title(df_numeric[col], col)\n    plt.title(f\"Distribution of {title} (Log Transformed)\" if df_numeric[col].max() > 1e5 else f\"Distribution of {col}\")\n    plt.xlabel(col)\n    \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:28:25.783296Z","iopub.execute_input":"2024-11-22T12:28:25.783625Z","iopub.status.idle":"2024-11-22T12:28:32.412160Z","shell.execute_reply.started":"2024-11-22T12:28:25.783593Z","shell.execute_reply":"2024-11-22T12:28:32.410899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Before starting with the correlation matrix, we can quickly find out which columns are similar and get information about the distributions in the tables. For example:\n    # Most tracks are around 3.5 minutes long.\n    # Almost all of the tracks are not live recordings.\n\n# By the way, the multiples are indicated in the title of the table (M: Million, B: Billion).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:49:21.978786Z","iopub.execute_input":"2024-11-22T12:49:21.979296Z","iopub.status.idle":"2024-11-22T12:49:21.984564Z","shell.execute_reply.started":"2024-11-22T12:49:21.979251Z","shell.execute_reply":"2024-11-22T12:49:21.983321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Good. Now let's create and visualize a correlation matrix.\n# With the correlation matrix, we can see the correlations between the columns and decide on the values to use in the analysis.\n\n# Let's create the correlation matrix with the df_numeric column.\ncorr_matrix = df_numeric.corr()\n\n# Here we visualize the correlation matrix.\n# Using a heat map in a correlation matrix facilitates analysis and insight.\nplt.figure(figsize = (10,8))\nsns.heatmap(corr_matrix, annot = True, cmap = \"Spectral\", center = 0, fmt = \".2f\")\nplt.title(\"Correlation Matrix Heat Map\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:28:32.413659Z","iopub.execute_input":"2024-11-22T12:28:32.414094Z","iopub.status.idle":"2024-11-22T12:28:33.227998Z","shell.execute_reply.started":"2024-11-22T12:28:32.414046Z","shell.execute_reply":"2024-11-22T12:28:33.226718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ****Examination of Correlation Matrix****\n\n### When we examine the correlation matrix, we see a strong positive correlation between the Likes column and the Views column, while there is a moderate correlation with the Stream and Comments columns. Based on this, it can be said that songs that are listened to more on Youtube receive more likes. I see that songs with a higher number of likes have a higher number of streams on Spotify. \n\n### There is also a strong positive correlation between Energy and Loudness, meaning that songs with higher energy can be considered as louder songs. On the contrary, we observe a moderate negative correlation between Energy and Acousticity.  Based on this, we can infer that noisy songs are far from being acoustic, and the matrix confirms this idea.\n\n### Based on all these, I will use the Views column for my analysis among the Stream, Likes and Views columns, as I find the correlation between them sufficient. I will also choose the Energy column between the Energy and Loudness columns.","metadata":{}},{"cell_type":"markdown","source":"# ****Data Analys****","metadata":{}},{"cell_type":"code","source":"# Energy, Valence, Speechiness, Tempo and Key analysis based on total Views.\n\n# Before starting the analysis, let's create Bins for columns according to the column information given in the entry. \nenergy_bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\nvalence_bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\nspeechiness_bins = [0, 0.33, 0.66, 1]\ntempo_bins = [0, 80, 120, 160, 200]\n\n# Grouping the features into bins\ndf[\"Energy_group\"] = pd.cut(df[\"Energy\"], bins = energy_bins, labels = [\"0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1\"])\ndf[\"Valence_group\"] = pd.cut(df[\"Valence\"], bins = valence_bins, labels = [\"0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1\"])\ndf[\"Speechiness_group\"] = pd.cut(df[\"Speechiness\"], bins = speechiness_bins, labels = [\"0-0.33\", \"0.33-0.66\", \"0.66-1\"])\ndf[\"Tempo_group\"] = pd.cut(df[\"Tempo\"], bins = tempo_bins, labels = [\"Less Than 80\", \"80-120\", \"120-160\", \"160-200\"])\ndf[\"Key_group\"] = pd.cut(df[\"Key\"], bins = range(0, 12), labels = [f\"{i}\" for i in range(11)])\n\n# Now that we have done the grouping, let's start visualization\nplt.figure(figsize = (16, 12))\n\n# Energy\nplt.subplot(2, 3, 1)\ndf.groupby(\"Energy_group\")[\"Views\"].sum().plot(kind = \"bar\", color = \"midnightblue\")\nplt.title(\"Total Views by Energy Group\")\nplt.xlabel(\"Energy Group\")\nplt.ylabel(\"Total Views\")\n\n# Valence\nplt.subplot(2, 3, 2)\ndf.groupby(\"Valence_group\")[\"Views\"].sum().plot(kind = \"bar\", color = \"maroon\")\nplt.title(\"Total Views by Valence Group\")\nplt.xlabel(\"Valence Group\")\nplt.ylabel(\"Total Views\")\n\n# Speechiness\nplt.subplot(2, 3, 3)\ndf.groupby(\"Speechiness_group\")[\"Views\"].sum().plot(kind = \"bar\", color = \"darkviolet\")\nplt.title(\"Total Views by Speechiness Group\")\nplt.xlabel(\"Speechiness Group\")\nplt.ylabel(\"Total Views\")\n\n# Tempo\nplt.subplot(2, 3, 4)\ndf.groupby(\"Tempo_group\")[\"Views\"].sum().plot(kind = \"bar\", color = \"orange\")\nplt.title(\"Total Views by Tempo Group\")\nplt.xlabel(\"Tempo Group (BPM)\")\nplt.ylabel(\"Total Views\")\n\n# Key\nplt.subplot(2, 3, 5)\ndf.groupby(\"Key_group\")[\"Views\"].sum().plot(kind = \"bar\", color = \"limegreen\")\nplt.title(\"Total Views by Key Group\")\nplt.xlabel(\"Key Group\")\nplt.ylabel(\"Total Views\")\nplt.xticks(rotation = 0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:28:33.229586Z","iopub.execute_input":"2024-11-22T12:28:33.230020Z","iopub.status.idle":"2024-11-22T12:28:34.530036Z","shell.execute_reply.started":"2024-11-22T12:28:33.229971Z","shell.execute_reply":"2024-11-22T12:28:34.528945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Music that is not very energetic but has above average energy has more views and therefore more likes and streams. \n    # If the person concerned is a songwriter, they can gain popularity faster by making songs in this style. \n    # If the relevant person is a Spotify or YouTube executive, this information can be used to take appropriate actions for the advertising campaign or sales policy.\n\n# The same goes for the Valance value. \n    # Positive songs that can be called cheerful and enthusiastic above average have more views, but songs with average and below average emotions are preferred more than the most positive or most negative tracks. \n    # In order not to take risks, it would be better to keep the emotional balance at the average. Highlighting such tracks will increase the number of views and streams.\n\n# When we examine the Speechiness group, the most preferred tracks are the ones where music is at the forefront and lyrics are few.\n\n# While the tempo values most preferred by people are in the 80-160 BPM range, the most preferred key is C (Do), followed by C# (Do#), F# (Fa#) and A# (La#) with almost equal data. \n    # With this data, we observe that listeners are more interested in certain tones. If we analyze these 3 keys, the results will make more sense. \n    # C# (Do#) creates a bright, lively and cheerful atmosphere. It is the key commonly used by Hüseyni Makam in Turkish Art Music and has a calming and melodic structure.\n    # F# (Fa#) offers peaceful and elegant tones and is commonly used in the Rast Makam in Turkish Art Music, which I also love to listen to and play on the violin. \n    # Finally, A# (La#) can be preferred to provide strong emotional intonations. It matches the Segah Makam in Turkish art music and creates a more melancholic mood.\n\n# In the light of all this information, the person concerned can choose according to the actions to be taken and achieve the best efficiency.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see the sum of View values based on Album_type with Donut Chart.\n\n# First, I group by creating the album_views variable\nalbum_views = df.groupby(\"Album_type\")[\"Views\"].sum()\n\n# Now let's create the Donut Chart.\nplt.figure(figsize = (8, 8))\n\nplt.pie(album_views, labels = album_views.index, autopct = \"%1.2f%%\", startangle = 135, colors = [\"maroon\", \"lime\", \"navy\"], wedgeprops = dict(width = 0.3))\nplt.title(\"Album Type Bazında View Oranı\", fontsize = 16)\nplt.legend(labels = [f\"{label}: {value:,.0f} views\" for label, value in zip(album_views.index, album_views)],\n           title = \"Album Type\", loc = \"upper left\", bbox_to_anchor = (1, 0, 0.5, 1))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:50:58.598336Z","iopub.execute_input":"2024-11-22T12:50:58.598812Z","iopub.status.idle":"2024-11-22T12:50:58.872188Z","shell.execute_reply.started":"2024-11-22T12:50:58.598772Z","shell.execute_reply":"2024-11-22T12:50:58.871133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We see that tracks released as albums have more views. \n# Even if all of the data we extracted from the dataset were singles, album tracks would still be dominant.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:25:14.161144Z","iopub.execute_input":"2024-11-22T13:25:14.161584Z","iopub.status.idle":"2024-11-22T13:25:14.166968Z","shell.execute_reply.started":"2024-11-22T13:25:14.161547Z","shell.execute_reply":"2024-11-22T13:25:14.165666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's check on the pie chart how the total number of views is affected in Licensed and Official cases.\n\n# I set it to show all percentages with the special autopct function.\ndef custom_autopct(pct, colors):\n    return f\"{pct:.2f}%\"\n\n# Let's calculate total Views by grouping Licensed and official_video columns\nlicensed_views = df.groupby(\"Licensed\")[\"Views\"].sum()\nofficial_video_views = df.groupby(\"official_video\")[\"Views\"].sum()\n\n# Editing charts size\nplt.figure(figsize=(16, 8))\n\n# Let's create Licensed pie chart\nplt.subplot(1, 2, 1)\ncolors_licensed = [\"lightsalmon\", \"maroon\"]\nwedges, texts, autotexts = plt.pie(\n    licensed_views,\n    labels=licensed_views.index,\n    autopct=lambda pct: custom_autopct(pct, colors_licensed),\n    startangle=90,\n    colors=colors_licensed,\n)\n# To make the percentage expressions more visible in the chart, I set their colors according to the background color.\nfor text, color in zip(autotexts, colors_licensed):\n    text.set_color(\"white\" if color == \"maroon\" else \"black\")\nplt.title(\"Licensed - Total Views\")\nplt.legend(\n    [f\"{label}: {views:,} Views\" for label, views in zip(licensed_views.index, licensed_views)],\n    loc=\"best\",\n)\n\n# Creating official_video pie chart\nplt.subplot(1, 2, 2)\ncolors_official = [\"lime\", \"midnightblue\"]\nwedges, texts, autotexts = plt.pie(\n    official_video_views,\n    labels=official_video_views.index,\n    autopct=lambda pct: custom_autopct(pct, colors_official),\n    startangle=90,\n    colors=colors_official,\n)\n# I do the same for the percentage expressions to more visible.\nfor text, color in zip(autotexts, colors_official):\n    text.set_color(\"white\" if color == \"midnightblue\" else \"black\")\nplt.title(\"Official Video - Total Views\")\nplt.legend(\n    [f\"{label}: {views:,} Views\" for label, views in zip(official_video_views.index, official_video_views)],\n    loc=\"best\",\n)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:43:38.198263Z","iopub.execute_input":"2024-11-22T13:43:38.198663Z","iopub.status.idle":"2024-11-22T13:43:38.644261Z","shell.execute_reply.started":"2024-11-22T13:43:38.198630Z","shell.execute_reply":"2024-11-22T13:43:38.642878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# As a result of this information, the importance of the part manufacturers producing official content and the importance of obtaining copyrights can be emphasized. \n# This information can be presented supporting with graphic.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top 10 Anaysis","metadata":{}},{"cell_type":"code","source":"# Now, Let's examine Top 10s. Prizes for you Top 10s! :)\n\n# Top 10 Artist by Views\ntop10_artist_views = df.groupby(\"Artist\")[\"Views\"].sum().nlargest(10)\n\n# We will analyze the Top 10s on a horizontal bar chart \nplt.figure(figsize = (10, 6))\nsns.barplot(x = top10_artist_views.values, y = top10_artist_views.index, palette = \"viridis\")\nplt.title(\"Top 10 Artists by View Count\")\nplt.xlabel(\"Views\")\nplt.ylabel(\"Artist\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:52:43.959678Z","iopub.execute_input":"2024-11-22T13:52:43.960140Z","iopub.status.idle":"2024-11-22T13:52:44.270265Z","shell.execute_reply.started":"2024-11-22T13:52:43.960101Z","shell.execute_reply":"2024-11-22T13:52:44.269181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top 10 Track by Views\ntop10_track_views = df.groupby(\"Track\")[\"Views\"].sum().nlargest(10)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x = top10_track_views.values, y = top10_track_views.index, palette = \"inferno\")\nplt.title(\"Top 10 Tracks by View Count\")\nplt.xlabel(\"Views\")\nplt.ylabel(\"Track\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:05:13.449869Z","iopub.execute_input":"2024-11-22T14:05:13.450329Z","iopub.status.idle":"2024-11-22T14:05:13.783485Z","shell.execute_reply.started":"2024-11-22T14:05:13.450293Z","shell.execute_reply":"2024-11-22T14:05:13.782278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top 10 Channel by Views\ntop10_channel_views = df.groupby(\"Channel\")[\"Views\"].sum().nlargest(10)\n\nplt.figure(figsize = (10, 6))\nsns.barplot(x = top10_channel_views.values, y = top10_channel_views.index, palette = \"viridis\")\nplt.title(\"Top 10 Channels by View Count\")\nplt.xlabel(\"Views\")\nplt.ylabel(\"Channel\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:05:33.307306Z","iopub.execute_input":"2024-11-22T14:05:33.307720Z","iopub.status.idle":"2024-11-22T14:05:33.609879Z","shell.execute_reply.started":"2024-11-22T14:05:33.307676Z","shell.execute_reply":"2024-11-22T14:05:33.608496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****Overall Analysis****\n### We can announce the Top 10 regularly on Youtube and Spotify pages and we can also create a small reward system for these achievements. \n### Prizes or badges can be awarded to those with a total of 21 awards (this limit can be adjusted). \n### We could even scale up and make smaller channels more competitive among themselves. \n### This can be a move that can increase sharing and interaction by creating competition between channels or artists. \n### In this way, channels will advertise themselves and this will result in more users.","metadata":{}}]}